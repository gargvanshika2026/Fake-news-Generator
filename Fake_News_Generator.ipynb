{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "2ccc3d6378c14b6dac9c0ab62d30477e",
            "e50cfbc3cfd8436d94a98cb488fa10af",
            "98461d6d0b0543e7959c6f3296690294",
            "646fac91cb504966bbf627ba06669f3d",
            "fdc46684004144d49926ac7e545cc1bf",
            "521cf564a2014a1d9d9dfa51b7ff7376",
            "18645daaad204041870d2986ff766dcc"
          ]
        },
        "id": "kg23Lc20iuKm",
        "outputId": "bdbb17c8-d3a7-42fe-efa6-7e977887389a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
            "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
            "You are not authenticated with the Hugging Face Hub in this notebook.\n",
            "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2ccc3d6378c14b6dac9c0ab62d30477e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e50cfbc3cfd8436d94a98cb488fa10af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "98461d6d0b0543e7959c6f3296690294",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "646fac91cb504966bbf627ba06669f3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fdc46684004144d49926ac7e545cc1bf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "521cf564a2014a1d9d9dfa51b7ff7376",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "18645daaad204041870d2986ff766dcc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "import torch\n",
        "\n",
        "# Load GPT-2 base model and tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "model.eval()\n",
        "\n",
        "def generate_fake_headline(prompt=\"Breaking: \"):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs[\"input_ids\"], max_length=30, num_return_sequences=1, do_sample=True)\n",
        "    headline = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return headline.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "YgACNQbViyp4"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Paste your actual token below (keep it private!)\n",
        "login(\"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\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "3260008d9fcf4529bb7a181fa36870ef",
            "e71d958cba7a412d9738b037a95040e0",
            "b31cd183fb23410c8c23d13538400145",
            "9f7869b75b67405aa35c859c90306451"
          ]
        },
        "id": "rXUTXYyvi1dA",
        "outputId": "885ccb29-6e2d-464a-fde9-f972e05105dd"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3260008d9fcf4529bb7a181fa36870ef",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e71d958cba7a412d9738b037a95040e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b31cd183fb23410c8c23d13538400145",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9f7869b75b67405aa35c859c90306451",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load sentiment analysis pipeline (publicly available model)\n",
        "classifier = pipeline(\"text-classification\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def detect_fake_news(headline):\n",
        "    result = classifier(headline)[0]\n",
        "    sentiment = result[\"label\"]\n",
        "    confidence = result[\"score\"]\n",
        "\n",
        "    # Simulate: POSITIVE ‚Üí REAL, NEGATIVE ‚Üí FAKE\n",
        "    if sentiment == \"POSITIVE\":\n",
        "        label = \"REAL\"\n",
        "    else:\n",
        "        label = \"FAKE\"\n",
        "\n",
        "    return f\"{label} ({confidence:.2f})\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 648
        },
        "id": "NkVJ-3HQi4DJ",
        "outputId": "671ee7ae-485f-4db1-8706-623b87e10590"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://50f5c220bd83bf1cdb.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://50f5c220bd83bf1cdb.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "\n",
        "import gradio as gr\n",
        "\n",
        "# Combine into Gradio app\n",
        "def fake_news_pipeline(prompt):\n",
        "    generated_headline = generate_fake_headline(prompt)\n",
        "    detection_result = detect_fake_news(generated_headline)\n",
        "    return generated_headline, detection_result\n",
        "\n",
        "gr.Interface(\n",
        "    fn=fake_news_pipeline,\n",
        "    inputs=gr.Textbox(label=\"Enter Prompt for Fake News Headline\"),\n",
        "    outputs=[\n",
        "        gr.Textbox(label=\"Generated Fake News Headline\"),\n",
        "        gr.Textbox(label=\"Detection Result (Fake or Real)\")\n",
        "    ],\n",
        "    title=\"üì∞ Fake News Generator & Detector\",\n",
        "    description=\"Generate a fake news-style headline using GPT-2 and classify it as FAKE or REAL using a sentiment-mapped BERT model.\\n‚ö†Ô∏è Educational purpose only!\"\n",
        ").launch()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8s3xFeXHi60A"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}